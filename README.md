# üîç TAMX: An Efficient and Interactive Implementation of Token Activation Maps for VLM Interpretability

[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)
[![Qwen3-VL](https://img.shields.io/badge/Qwen3-VL-purple)](https://github.com/QwenLM/Qwen3-VL)
[![Demo](https://img.shields.io/badge/View-demo-green)](https://htmlpreview.github.io/?https://github.com/joinn99/tamx/blob/main/asset/output/generate_vis.html) 

<p align="center" width="100%">
    <kbd><img width="100%" src="https://raw.githubusercontent.com/Joinn99/TAMX/main/asset/TAMX.jpg"> </kbd>
</p>


TAMX is an efficient implementation of an ICCV 2025 paper "[Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)". It is a high-performance, lightweight library designed to peer into the "black box" of Vision Large Language Models (VLMs), such as Qwen3-VL. By mapping text token generation back to specific visual and textual inputs, TAM provides a clear window into how models process multimodal information.

---

## üåü Key Features

- **üéØ Precision Visual Mapping**: Generate high-resolution relevance maps for images and videos at a per-token level.
- **üõ°Ô∏è Error-Corrected Interpretability (ECI)**: Advanced noise reduction that eliminates spurious correlations by projecting activations against context tokens.
- **üé¨ Full Video Support**: Seamlessly handles temporal dimensions, allowing you to see which frames influenced a specific word.
- **üìä Interactive Visualization**: Export dynamic HTML reports where you can click tokens to see their corresponding activation maps.
- **‚ö° Zero Overhead**: Designed for research and production, with minimal dependencies and efficient tensor operations.

---

## üïπÔ∏è Interactive Example

Experience TAM in action! Below is a preview of the interactive dashboard generated by TAM. 

<p align="center" width="100%">
    <kbd><img width="100%" src="https://raw.githubusercontent.com/Joinn99/TAMX/main/asset/Demo.gif"> </kbd>
</p>

> **[üöÄ View Interactive Demo](https://htmlpreview.github.io/?https://github.com/joinn99/tamx/blob/main/asset/output/generate_vis.html)**

*(Note: The demo above is a static HTML export. For the best experience, click on tokens in the chat to see the heatmaps update in real-time.)*

### How it works:
1. **Interactive Chat**: Hover over or click on any token in the model's response.
2. **Vision Map**: Watch the heatmaps update in real-time on the associated images or video frames.
3. **Text Relevance**: See which previous words in the prompt were most "attended to" for that specific token generation.

---

## üöÄ Quick Start

### üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/joinn99/tamx.git
cd TAM

# Install core dependencies
uv sync

# (Optional) Install visualization utilities for Qwen-VL
uv sync --all-extras
```

### üõ†Ô∏è Basic Usage

Get up and running with just a few lines of code:

```python
import torch
from tamx.core import compute_tam

# Load your model and data...
results = compute_tam(
    hidden_states,                   # [SequenceLength-1, HiddenDim]
    input_ids,                       # [SequenceLength]
    lm_head_weight,                  # [Vocab, HiddenDim]
    candidate_token_ids,             # [NumGeneratedTokens, NumCandidates] (Optional)
    image_grid_thw,                  # [NumImages, 3 (Time, Height, Width)] (Optional)
    video_grid_thw,                  # [NumVideos, 3 (Time, Height, Width)] (Optional)
    apply_eci=True,                  # Enable Estimated Causal Inference (ECI)
    apply_filter=True,               # Apply Ranking-based Smoothing Filter
    kernel_size=3,                   # Kernel Size for the Smoothing Filter
)

```

---

## üé® Visualization

TAM generates interactive HTML dashboards that make model analysis intuitive. You can generate your own reports using the following commands:

```bash
# Run the sample visualization script
export MODEL_PATH="Qwen/Qwen3-VL-2B-Instruct"
python example/offline/encode.py        # Encoding hidden states for generated tokens
python example/offline/generate.py      # Generating text together with candidates and hidden states
python visualization.py                 # Visualizing the results
```

The output will be a standalone HTML file (like our [interactive example](#üïπÔ∏è-interactive-example)) that you can share or host easily.

---

## üåê Interactive Web App

```bash
export MODEL_PATH="Qwen/Qwen3-VL-2B-Instruct"

python -m uvicorn webapp.server:app --host 0.0.0.0 --port 8016
```

Then open your browser and go to http://localhost:8016.


## üìÅ Repository Structure

| Folder/File | Description |
|:---|:---|
| `tamx/` | Core logic and computation engines |
| `tamx/vis/` | HTML/JS/CSS for visualization |
| `webapp/` | Full-stack FastAPI interface for real-time model probing |

---

## Supported Models

Currently, TAMX supports all [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl) series models and [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl) series models.

---

## üìÑ License

Distributed under the **Apache-2.0 License**. See `LICENSE` for more information.

---

## üì¨ Thanks

- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)
- [TAM](https://github.com/xmed-lab/TAM)
